{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv')\ndf.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-15T17:32:52.895722Z","iopub.execute_input":"2022-01-15T17:32:52.896467Z","iopub.status.idle":"2022-01-15T17:32:55.417749Z","shell.execute_reply.started":"2022-01-15T17:32:52.896334Z","shell.execute_reply":"2022-01-15T17:32:55.416443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Shape - {df.shape}\")\nprint(f\"columns - {df.columns}\")","metadata":{"execution":{"iopub.status.busy":"2022-01-15T17:32:55.41989Z","iopub.execute_input":"2022-01-15T17:32:55.420245Z","iopub.status.idle":"2022-01-15T17:32:55.429621Z","shell.execute_reply.started":"2022-01-15T17:32:55.4202Z","shell.execute_reply":"2022-01-15T17:32:55.428586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom collections import Counter\nimport string\nimport re\nfrom tqdm import tqdm\nfrom textblob import TextBlob\nfrom urllib.error import HTTPError\nimport nltk\nfrom collections import defaultdict\nfrom nltk.corpus import wordnet as wn\nfrom nltk import word_tokenize, pos_tag\n","metadata":{"execution":{"iopub.status.busy":"2022-01-15T17:32:55.431105Z","iopub.execute_input":"2022-01-15T17:32:55.431668Z","iopub.status.idle":"2022-01-15T17:32:57.255628Z","shell.execute_reply.started":"2022-01-15T17:32:55.43162Z","shell.execute_reply":"2022-01-15T17:32:57.254828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#preprocessing:\npunc = string.punctuation + '•'\ncontractions_dict = {\"ain't\": 'are not',\n \"'s\": ' is',\n \"aren't\": 'are not',\n \"can't\": 'cannot',\n \"can't've\": 'cannot have',\n \"'cause\": 'because',\n \"could've\": 'could have',\n \"couldn't\": 'could not',\n \"couldn't've\": 'could not have',\n \"didn't\": 'did not',\n \"doesn't\": 'does not',\n \"don't\": 'do not',\n \"hadn't\": 'had not',\n \"hadn't've\": 'had not have',\n \"hasn't\": 'has not',\n \"haven't\": 'have not',\n \"he'd\": 'he would',\n \"he'd've\": 'he would have',\n \"he'll\": 'he will',\n \"he'll've\": 'he will have',\n \"how'd\": 'how did',\n \"how'd'y\": 'how do you',\n \"how'll\": 'how will',\n \"i'd\": 'i would',\n \"i'd've\": 'i would have',\n \"i'll\": 'i will',\n \"i'll've\": 'i will have',\n \"i'm\": 'i am',\n \"i've\": 'i have',\n \"isn't\": 'is not',\n \"it'd\": 'it would',\n \"it'd've\": 'it would have',\n \"it'll\": 'it will',\n \"it'll've\": 'it will have',\n \"let's\": 'let us',\n \"ma'am\": 'madam',\n \"mayn't\": 'may not',\n \"might've\": 'might have',\n \"mightn't\": 'might not',\n \"mightn't've\": 'might not have',\n \"must've\": 'must have',\n \"mustn't\": 'must not',\n \"mustn't've\": 'must not have',\n \"needn't\": 'need not',\n \"needn't've\": 'need not have',\n \"o'clock\": 'of the clock',\n \"oughtn't\": 'ought not',\n \"oughtn't've\": 'ought not have',\n \"shan't\": 'shall not',\n \"sha'n't\": 'shall not',\n \"shan't've\": 'shall not have',\n \"she'd\": 'she would',\n \"she'd've\": 'she would have',\n \"she'll\": 'she will',\n \"she'll've\": 'she will have',\n \"should've\": 'should have',\n \"shouldn't\": 'should not',\n \"shouldn't've\": 'should not have',\n \"so've\": 'so have',\n \"that'd\": 'that would',\n \"that'd've\": 'that would have',\n \"there'd\": 'there would',\n \"there'd've\": 'there would have',\n \"they'd\": 'they would',\n \"they'd've\": 'they would have',\n \"they'll\": 'they will',\n \"they'll've\": 'they will have',\n \"they're\": 'they are',\n \"they've\": 'they have',\n \"to've\": 'to have',\n \"wasn't\": 'was not',\n \"we'd\": 'we would',\n \"we'd've\": 'we would have',\n \"we'll\": 'we will',\n \"we'll've\": 'we will have',\n \"we're\": 'we are',\n \"we've\": 'we have',\n \"weren't\": 'were not',\n \"what'll\": 'what will',\n \"what'll've\": 'what will have',\n \"what're\": 'what are',\n \"what've\": 'what have',\n \"when've\": 'when have',\n \"where'd\": 'where did',\n \"where've\": 'where have',\n \"who'll\": 'who will',\n \"who'll've\": 'who will have',\n \"who've\": 'who have',\n \"why've\": 'why have',\n \"will've\": 'will have',\n \"won't\": 'will not',\n \"won't've\": 'will not have',\n \"would've\": 'would have',\n \"wouldn't\": 'would not',\n \"wouldn't've\": 'would not have',\n \"y'all\": 'you all',\n \"y'all'd\": 'you all would',\n \"y'all'd've\": 'you all would have',\n \"y'all're\": 'you all are',\n \"y'all've\": 'you all have',\n \"you'd\": 'you would',\n \"you'd've\": 'you would have',\n \"you'll\": 'you will',\n \"you'll've\": 'you will have',\n \"you're\": 'you are',\n \"you've\": 'you have',\n' afaik ': ' as far as i know ',\n ' afk ': ' away from keyboard',\n ' asap ': ' as soon as possible',\n ' atk ': ' at the keyboard',\n ' atm ': ' at the moment',\n ' bak ': ' back at keyboard ',\n ' bbl ': ' be back later ',\n ' bbs ': ' be back soon ',\n ' bfn ': ' bye for now ',\n ' b4n ': ' bye for now ',\n ' brb ': ' be right back ',\n ' brt ': ' be right there ',\n ' btw ': ' by the way ',\n ' b4 ': ' before ',\n ' cu ': ' see you ',\n ' cul8r ': ' see you later ',\n ' cya ': ' see you ',\n ' faq ': ' frequently asked questions ',\n ' fc ': ' fingers crossed ',\n ' fwiw ': \" for what it is worth \",\n ' fyi ': ' for your information ',\n ' gal ': ' get a life ',\n ' gg ': ' good game ',\n ' gn ': ' good night ',\n 'gmta': 'great minds think alike',\n ' gr8 ': ' great! ',\n ' g9 ': ' genius ',\n ' ic ': ' i see ',\n ' icq ': ' i seek you ',\n 'ilu': ' i love you',\n 'imho': ' in my humble opinion',\n ' imo ': ' in my opinion ',\n ' iow ': ' in other words ',\n 'irl': 'in real life',\n ' ldr ': ' long distance relationship ',\n 'lmao': ' laugh my ass off',\n ' lol ': ' laughing out loud ',\n 'ltns': ' long time no see',\n ' l8r ': ' later ',\n ' mte ': ' my thoughts exactly ',\n ' m8 ': ' mate ',\n ' nrn ': ' no reply necessary ',\n ' oic ': ' oh i see ',\n ' pita ': ' pain in the ass ',\n ' prt ': ' party ',\n ' prw ': ' parents are watching ',\n 'qpsa?': 'que pasa?',\n 'rofl': 'rolling on the floor laughing',\n 'roflol': 'rolling on the floor laughing out loud',\n 'rotflmao': 'rolling on the floor laughing my ass off',\n ' sk8 ': ' skate ',\n ' stats ': ' your sex and age ',\n ' asl ': ' age, sex, location ',\n ' thx ': ' thank you ',\n 'ttfn': 'ta-ta for now!',\n 'ttyl': ' talk to you later',\n ' u ': ' you ',\n 'u2': ' you too',\n 'u4e': ' yours for ever',\n ' wb ': ' welcome back ',\n 'wtf': ' what the fuck',\n 'wtg': ' way to go!',\n 'wuf': ' where are you from?',\n ' w8 ': ' wait ',\n' pov ':' point of view '}\n\n# Regular expression for finding contractions\ncontractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n# Function for expanding contractions\ndef expand_contractions(text,contractions_dict=contractions_dict):\n    def replace(match):\n        return contractions_dict[match.group(0)]\n    return contractions_re.sub(replace, text)\n\n\ndef text_cleaning(txt):\n    \n    txt = txt.lower()\n    \n    #html\n    txt = ' '.join(['' if 'http' in words or 'www' in words or '.jpeg' in words or '.jpg' in words or '.png' in words or '.zip' in words or 'wikipeida_talk' in words or '@' in words or '•' in words or '&' in words or '\\n' in words else words for words in txt.split() ]) #html .jpg .png zip wikipeida_Talk removal    \n      \n    #mail\n    pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n    match = re.findall(pattern, 'This is my mail karthik123@gmail.com note it')\n    txt = txt.replace(match[0],'') #mail removal code\n    \n    txt = ''.join([char for char in txt if char not in punc+'1234567890' ]) #Punctuation & IP removal\n    txt = re.sub(r'([*!?\\'])\\1\\1{2,}',r'\\1\\1\\1',txt)\n    txt = re.sub(r'([*!?\\']+)',r' \\1 ',txt)\n    txt = re.sub(r'([a-zA-Z])\\1{2,}\\b',r'\\1\\1', txt)    \n    txt = re.sub(r'([a-zA-Z])\\1\\1{2,}\\B',r'\\1\\1\\1',txt)\n    \n    # Expanding Contractions in the comments\n    txt = expand_contractions(txt)\n    \n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    txt = emoji_pattern.sub(r'', txt)\n    #lang = TextBlob(txt) \n    \n    #try:\n      #  if lang.detect_language() != 'en' and len(txt) < 4000 :\n       #     txt = GoogleTranslator(source='auto', target='en').translate(txt)\n            \n    #except HTTPError:\n     #   print(txt)\n        #txt = translation.text\n\n    return txt","metadata":{"execution":{"iopub.status.busy":"2022-01-15T17:32:57.257697Z","iopub.execute_input":"2022-01-15T17:32:57.257931Z","iopub.status.idle":"2022-01-15T17:32:57.295255Z","shell.execute_reply.started":"2022-01-15T17:32:57.257902Z","shell.execute_reply":"2022-01-15T17:32:57.294366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tqdm.pandas()\ndf['cleaned_text'] = df['comment_text'].progress_apply(lambda x:text_cleaning(x))","metadata":{"execution":{"iopub.status.busy":"2022-01-15T17:32:57.296742Z","iopub.execute_input":"2022-01-15T17:32:57.296969Z","iopub.status.idle":"2022-01-15T17:34:24.197814Z","shell.execute_reply.started":"2022-01-15T17:32:57.29694Z","shell.execute_reply":"2022-01-15T17:34:24.196923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-15T17:34:24.199205Z","iopub.execute_input":"2022-01-15T17:34:24.199463Z","iopub.status.idle":"2022-01-15T17:34:24.215004Z","shell.execute_reply.started":"2022-01-15T17:34:24.199431Z","shell.execute_reply":"2022-01-15T17:34:24.214025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize(text):\n        token = nltk.word_tokenize(text)\n        return token\n\ndf['Tokenized_text'] = df['cleaned_text'].progress_apply(lambda x : tokenize(x))\n\nstpw = nltk.corpus.stopwords.words('english')\ndef rmstp(text):\n    tt = [char.lower() for char in text if char not in stpw ]\n    return tt\ndf['Tokenized_text'] = df['Tokenized_text'].progress_apply(lambda x:rmstp(x))\ndf.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T17:34:24.217013Z","iopub.execute_input":"2022-01-15T17:34:24.217408Z","iopub.status.idle":"2022-01-15T17:36:31.039493Z","shell.execute_reply.started":"2022-01-15T17:34:24.217362Z","shell.execute_reply":"2022-01-15T17:36:31.038652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tag_map = defaultdict(lambda : wn.NOUN)\ntag_map['J'] = wn.ADJ\ntag_map['V'] = wn.VERB\ntag_map['R'] = wn.ADV\nfrom nltk.stem import WordNetLemmatizer \nlemmatizer = WordNetLemmatizer() \ndef rmstmer(text):\n    tt = [lemmatizer.lemmatize(char,tag_map[j[0]]) for char,j in pos_tag(text)]\n    return tt\ndf['lemmatized'] = df['Tokenized_text'].progress_apply(lambda x:rmstmer(x))\nprint(df.head())","metadata":{"execution":{"iopub.status.busy":"2022-01-15T17:36:31.040809Z","iopub.execute_input":"2022-01-15T17:36:31.041066Z","iopub.status.idle":"2022-01-15T17:45:12.02774Z","shell.execute_reply.started":"2022-01-15T17:36:31.041034Z","shell.execute_reply":"2022-01-15T17:45:12.026717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['lemmatized_text'] = [\" \".join(i) for i in df['lemmatized']]\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-15T17:45:12.02893Z","iopub.execute_input":"2022-01-15T17:45:12.029183Z","iopub.status.idle":"2022-01-15T17:45:12.400413Z","shell.execute_reply.started":"2022-01-15T17:45:12.029139Z","shell.execute_reply":"2022-01-15T17:45:12.39975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\ndef word_cloud_generator(df):\n    comment_words = ''\n    stopwords = set(STOPWORDS)\n\n    # iterate through the csv file\n    for val in df.lemmatized_text:\n\n        # typecaste each val to string\n        val = str(val)\n\n        # split the value\n        tokens = val.split()\n\n        # Converts each token into lowercase\n        for i in range(len(tokens)):\n            tokens[i] = tokens[i].lower()\n\n        comment_words += \" \".join(tokens)+\" \"\n\n    wordcloud = WordCloud(width = 800, height = 800,\n                    background_color ='white',\n                    stopwords = stopwords,\n                    min_font_size = 10).generate(comment_words)\n\n    # plot the WordCloud image                      \n    plt.figure(figsize = (10, 10), facecolor = None)\n    plt.imshow(wordcloud)\n    plt.axis(\"off\")\n    plt.tight_layout(pad = 0)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-15T17:45:12.402986Z","iopub.execute_input":"2022-01-15T17:45:12.403677Z","iopub.status.idle":"2022-01-15T17:45:12.463224Z","shell.execute_reply.started":"2022-01-15T17:45:12.403619Z","shell.execute_reply":"2022-01-15T17:45:12.462208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#'toxic', 'severe_toxic', 'obscene', 'threat',\n#       'insult', 'identity_hate'\ndf_toxic = df[df['toxic'] == 1]\ndf_severe_toxic = df[df['severe_toxic'] == 1]\ndf_obscene = df[df['obscene'] == 1]\ndf_threat = df[df['threat'] == 1]\ndf_insult = df[df['insult'] == 1]\ndf_identity_hate = df[df['identity_hate'] == 1]","metadata":{"execution":{"iopub.status.busy":"2022-01-15T17:45:12.464456Z","iopub.execute_input":"2022-01-15T17:45:12.464699Z","iopub.status.idle":"2022-01-15T17:45:12.584137Z","shell.execute_reply.started":"2022-01-15T17:45:12.464668Z","shell.execute_reply":"2022-01-15T17:45:12.583492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(\"Category - Toxic\")\nword_cloud_generator(df_toxic)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T17:45:12.585814Z","iopub.execute_input":"2022-01-15T17:45:12.586108Z","iopub.status.idle":"2022-01-15T17:45:19.168391Z","shell.execute_reply.started":"2022-01-15T17:45:12.586074Z","shell.execute_reply":"2022-01-15T17:45:19.167336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(\"Category - Severe Toxic\")\nword_cloud_generator(df_severe_toxic)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T17:45:19.170137Z","iopub.execute_input":"2022-01-15T17:45:19.170575Z","iopub.status.idle":"2022-01-15T17:45:20.857122Z","shell.execute_reply.started":"2022-01-15T17:45:19.170543Z","shell.execute_reply":"2022-01-15T17:45:20.855993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(\"Category - Obscene\")\nword_cloud_generator(df_obscene)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T17:45:20.858459Z","iopub.execute_input":"2022-01-15T17:45:20.858699Z","iopub.status.idle":"2022-01-15T17:45:24.767455Z","shell.execute_reply.started":"2022-01-15T17:45:20.858669Z","shell.execute_reply":"2022-01-15T17:45:24.766639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(\"Category - threat\")\nword_cloud_generator(df_threat)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T17:45:24.769074Z","iopub.execute_input":"2022-01-15T17:45:24.769379Z","iopub.status.idle":"2022-01-15T17:45:26.916607Z","shell.execute_reply.started":"2022-01-15T17:45:24.769337Z","shell.execute_reply":"2022-01-15T17:45:26.915635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(\"Category - insult\")\nword_cloud_generator(df_insult)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T17:45:26.917706Z","iopub.execute_input":"2022-01-15T17:45:26.917924Z","iopub.status.idle":"2022-01-15T17:45:30.402092Z","shell.execute_reply.started":"2022-01-15T17:45:26.917896Z","shell.execute_reply":"2022-01-15T17:45:30.401476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(\"Category - identity_hate\")\nword_cloud_generator(df_identity_hate)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T17:45:30.40307Z","iopub.execute_input":"2022-01-15T17:45:30.40356Z","iopub.status.idle":"2022-01-15T17:45:32.86074Z","shell.execute_reply.started":"2022-01-15T17:45:30.403525Z","shell.execute_reply":"2022-01-15T17:45:32.859846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}