{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddac8b33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-09T18:41:09.302780Z",
     "iopub.status.busy": "2022-01-09T18:41:09.301525Z",
     "iopub.status.idle": "2022-01-09T18:41:11.107898Z",
     "shell.execute_reply": "2022-01-09T18:41:11.108531Z",
     "shell.execute_reply.started": "2022-01-09T17:47:49.889689Z"
    },
    "papermill": {
     "duration": 1.824854,
     "end_time": "2022-01-09T18:41:11.108911",
     "exception": false,
     "start_time": "2022-01-09T18:41:09.284057",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from googletrans import Translator, constants\n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "#from deep_translator import GoogleTranslator\n",
    "#from langdetect import detect,LangDetectException\n",
    "from textblob import TextBlob\n",
    "from urllib.error import HTTPError\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de7cb2ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-09T18:41:11.137592Z",
     "iopub.status.busy": "2022-01-09T18:41:11.136527Z",
     "iopub.status.idle": "2022-01-09T18:41:11.140625Z",
     "shell.execute_reply": "2022-01-09T18:41:11.141115Z",
     "shell.execute_reply.started": "2022-01-09T18:11:46.877246Z"
    },
    "papermill": {
     "duration": 0.019987,
     "end_time": "2022-01-09T18:41:11.141309",
     "exception": false,
     "start_time": "2022-01-09T18:41:11.121322",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "punc = string.punctuation + '•'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca1cf944",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-09T18:41:11.168883Z",
     "iopub.status.busy": "2022-01-09T18:41:11.167741Z",
     "iopub.status.idle": "2022-01-09T18:41:11.195954Z",
     "shell.execute_reply": "2022-01-09T18:41:11.196537Z",
     "shell.execute_reply.started": "2022-01-09T17:48:05.892175Z"
    },
    "papermill": {
     "duration": 0.043737,
     "end_time": "2022-01-09T18:41:11.196715",
     "exception": false,
     "start_time": "2022-01-09T18:41:11.152978",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "contractions_dict = {\"ain't\": 'are not',\n",
    " \"'s\": ' is',\n",
    " \"aren't\": 'are not',\n",
    " \"can't\": 'cannot',\n",
    " \"can't've\": 'cannot have',\n",
    " \"'cause\": 'because',\n",
    " \"could've\": 'could have',\n",
    " \"couldn't\": 'could not',\n",
    " \"couldn't've\": 'could not have',\n",
    " \"didn't\": 'did not',\n",
    " \"doesn't\": 'does not',\n",
    " \"don't\": 'do not',\n",
    " \"hadn't\": 'had not',\n",
    " \"hadn't've\": 'had not have',\n",
    " \"hasn't\": 'has not',\n",
    " \"haven't\": 'have not',\n",
    " \"he'd\": 'he would',\n",
    " \"he'd've\": 'he would have',\n",
    " \"he'll\": 'he will',\n",
    " \"he'll've\": 'he will have',\n",
    " \"how'd\": 'how did',\n",
    " \"how'd'y\": 'how do you',\n",
    " \"how'll\": 'how will',\n",
    " \"i'd\": 'i would',\n",
    " \"i'd've\": 'i would have',\n",
    " \"i'll\": 'i will',\n",
    " \"i'll've\": 'i will have',\n",
    " \"i'm\": 'i am',\n",
    " \"i've\": 'i have',\n",
    " \"isn't\": 'is not',\n",
    " \"it'd\": 'it would',\n",
    " \"it'd've\": 'it would have',\n",
    " \"it'll\": 'it will',\n",
    " \"it'll've\": 'it will have',\n",
    " \"let's\": 'let us',\n",
    " \"ma'am\": 'madam',\n",
    " \"mayn't\": 'may not',\n",
    " \"might've\": 'might have',\n",
    " \"mightn't\": 'might not',\n",
    " \"mightn't've\": 'might not have',\n",
    " \"must've\": 'must have',\n",
    " \"mustn't\": 'must not',\n",
    " \"mustn't've\": 'must not have',\n",
    " \"needn't\": 'need not',\n",
    " \"needn't've\": 'need not have',\n",
    " \"o'clock\": 'of the clock',\n",
    " \"oughtn't\": 'ought not',\n",
    " \"oughtn't've\": 'ought not have',\n",
    " \"shan't\": 'shall not',\n",
    " \"sha'n't\": 'shall not',\n",
    " \"shan't've\": 'shall not have',\n",
    " \"she'd\": 'she would',\n",
    " \"she'd've\": 'she would have',\n",
    " \"she'll\": 'she will',\n",
    " \"she'll've\": 'she will have',\n",
    " \"should've\": 'should have',\n",
    " \"shouldn't\": 'should not',\n",
    " \"shouldn't've\": 'should not have',\n",
    " \"so've\": 'so have',\n",
    " \"that'd\": 'that would',\n",
    " \"that'd've\": 'that would have',\n",
    " \"there'd\": 'there would',\n",
    " \"there'd've\": 'there would have',\n",
    " \"they'd\": 'they would',\n",
    " \"they'd've\": 'they would have',\n",
    " \"they'll\": 'they will',\n",
    " \"they'll've\": 'they will have',\n",
    " \"they're\": 'they are',\n",
    " \"they've\": 'they have',\n",
    " \"to've\": 'to have',\n",
    " \"wasn't\": 'was not',\n",
    " \"we'd\": 'we would',\n",
    " \"we'd've\": 'we would have',\n",
    " \"we'll\": 'we will',\n",
    " \"we'll've\": 'we will have',\n",
    " \"we're\": 'we are',\n",
    " \"we've\": 'we have',\n",
    " \"weren't\": 'were not',\n",
    " \"what'll\": 'what will',\n",
    " \"what'll've\": 'what will have',\n",
    " \"what're\": 'what are',\n",
    " \"what've\": 'what have',\n",
    " \"when've\": 'when have',\n",
    " \"where'd\": 'where did',\n",
    " \"where've\": 'where have',\n",
    " \"who'll\": 'who will',\n",
    " \"who'll've\": 'who will have',\n",
    " \"who've\": 'who have',\n",
    " \"why've\": 'why have',\n",
    " \"will've\": 'will have',\n",
    " \"won't\": 'will not',\n",
    " \"won't've\": 'will not have',\n",
    " \"would've\": 'would have',\n",
    " \"wouldn't\": 'would not',\n",
    " \"wouldn't've\": 'would not have',\n",
    " \"y'all\": 'you all',\n",
    " \"y'all'd\": 'you all would',\n",
    " \"y'all'd've\": 'you all would have',\n",
    " \"y'all're\": 'you all are',\n",
    " \"y'all've\": 'you all have',\n",
    " \"you'd\": 'you would',\n",
    " \"you'd've\": 'you would have',\n",
    " \"you'll\": 'you will',\n",
    " \"you'll've\": 'you will have',\n",
    " \"you're\": 'you are',\n",
    " \"you've\": 'you have',\n",
    "' afaik ': ' as far as i know ',\n",
    " ' afk ': ' away from keyboard',\n",
    " ' asap ': ' as soon as possible',\n",
    " ' atk ': ' at the keyboard',\n",
    " ' atm ': ' at the moment',\n",
    " ' bak ': ' back at keyboard ',\n",
    " ' bbl ': ' be back later ',\n",
    " ' bbs ': ' be back soon ',\n",
    " ' bfn ': ' bye for now ',\n",
    " ' b4n ': ' bye for now ',\n",
    " ' brb ': ' be right back ',\n",
    " ' brt ': ' be right there ',\n",
    " ' btw ': ' by the way ',\n",
    " ' b4 ': ' before ',\n",
    " ' cu ': ' see you ',\n",
    " ' cul8r ': ' see you later ',\n",
    " ' cya ': ' see you ',\n",
    " ' faq ': ' frequently asked questions ',\n",
    " ' fc ': ' fingers crossed ',\n",
    " ' fwiw ': \" for what it is worth \",\n",
    " ' fyi ': ' for your information ',\n",
    " ' gal ': ' get a life ',\n",
    " ' gg ': ' good game ',\n",
    " ' gn ': ' good night ',\n",
    " 'gmta': 'great minds think alike',\n",
    " ' gr8 ': ' great! ',\n",
    " ' g9 ': ' genius ',\n",
    " ' ic ': ' i see ',\n",
    " ' icq ': ' i seek you ',\n",
    " 'ilu': ' i love you',\n",
    " 'imho': ' in my humble opinion',\n",
    " ' imo ': ' in my opinion ',\n",
    " ' iow ': ' in other words ',\n",
    " 'irl': 'in real life',\n",
    " ' ldr ': ' long distance relationship ',\n",
    " 'lmao': ' laugh my ass off',\n",
    " ' lol ': ' laughing out loud ',\n",
    " 'ltns': ' long time no see',\n",
    " ' l8r ': ' later ',\n",
    " ' mte ': ' my thoughts exactly ',\n",
    " ' m8 ': ' mate ',\n",
    " ' nrn ': ' no reply necessary ',\n",
    " ' oic ': ' oh i see ',\n",
    " ' pita ': ' pain in the ass ',\n",
    " ' prt ': ' party ',\n",
    " ' prw ': ' parents are watching ',\n",
    " 'qpsa?': 'que pasa?',\n",
    " 'rofl': 'rolling on the floor laughing',\n",
    " 'roflol': 'rolling on the floor laughing out loud',\n",
    " 'rotflmao': 'rolling on the floor laughing my ass off',\n",
    " ' sk8 ': ' skate ',\n",
    " ' stats ': ' your sex and age ',\n",
    " ' asl ': ' age, sex, location ',\n",
    " ' thx ': ' thank you ',\n",
    " 'ttfn': 'ta-ta for now!',\n",
    " 'ttyl': ' talk to you later',\n",
    " ' u ': ' you ',\n",
    " 'u2': ' you too',\n",
    " 'u4e': ' yours for ever',\n",
    " ' wb ': ' welcome back ',\n",
    " 'wtf': ' what the fuck',\n",
    " 'wtg': ' way to go!',\n",
    " 'wuf': ' where are you from?',\n",
    " ' w8 ': ' wait ',\n",
    "' pov ':' point of view '}\n",
    "\n",
    "# Regular expression for finding contractions\n",
    "contractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
    "# Function for expanding contractions\n",
    "def expand_contractions(text,contractions_dict=contractions_dict):\n",
    "    def replace(match):\n",
    "        return contractions_dict[match.group(0)]\n",
    "    return contractions_re.sub(replace, text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f0b6fc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-09T18:41:11.225170Z",
     "iopub.status.busy": "2022-01-09T18:41:11.224101Z",
     "iopub.status.idle": "2022-01-09T18:41:11.235090Z",
     "shell.execute_reply": "2022-01-09T18:41:11.235673Z",
     "shell.execute_reply.started": "2022-01-09T17:58:36.810011Z"
    },
    "papermill": {
     "duration": 0.027888,
     "end_time": "2022-01-09T18:41:11.235886",
     "exception": false,
     "start_time": "2022-01-09T18:41:11.207998",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def text_cleaning(txt):\n",
    "    \n",
    "    txt = txt.lower()\n",
    "    \n",
    "    #html\n",
    "    txt = ' '.join(['' if 'http' in words or 'www' in words or '.jpeg' in words or '.jpg' in words or '.png' in words or '.zip' in words or 'wikipeida_talk' in words or '@' in words or '•' in words or '&' in words or '\\n' in words else words for words in txt.split() ]) #html .jpg .png zip wikipeida_Talk removal    \n",
    "      \n",
    "    #mail\n",
    "    pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "    match = re.findall(pattern, 'This is my mail karthik123@gmail.com note it')\n",
    "    txt = txt.replace(match[0],'') #mail removal code\n",
    "    \n",
    "    txt = ''.join([char for char in txt if char not in punc+'1234567890' ]) #Punctuation & IP removal\n",
    "    txt = re.sub(r'([*!?\\'])\\1\\1{2,}',r'\\1\\1\\1',txt)\n",
    "    txt = re.sub(r'([*!?\\']+)',r' \\1 ',txt)\n",
    "    txt = re.sub(r'([a-zA-Z])\\1{2,}\\b',r'\\1\\1', txt)    \n",
    "    txt = re.sub(r'([a-zA-Z])\\1\\1{2,}\\B',r'\\1\\1\\1',txt)\n",
    "    \n",
    "    # Expanding Contractions in the comments\n",
    "    txt = expand_contractions(txt)\n",
    "    \n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    txt = emoji_pattern.sub(r'', txt)\n",
    "    #lang = TextBlob(txt) \n",
    "    \n",
    "    #try:\n",
    "      #  if lang.detect_language() != 'en' and len(txt) < 4000 :\n",
    "       #     txt = GoogleTranslator(source='auto', target='en').translate(txt)\n",
    "            \n",
    "    #except HTTPError:\n",
    "     #   print(txt)\n",
    "        #txt = translation.text\n",
    "\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aef705bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-09T18:41:11.268984Z",
     "iopub.status.busy": "2022-01-09T18:41:11.268264Z",
     "iopub.status.idle": "2022-01-09T18:41:23.684682Z",
     "shell.execute_reply": "2022-01-09T18:41:23.685225Z",
     "shell.execute_reply.started": "2022-01-09T17:58:42.550958Z"
    },
    "papermill": {
     "duration": 12.437567,
     "end_time": "2022-01-09T18:41:23.685447",
     "exception": false,
     "start_time": "2022-01-09T18:41:11.247880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>target</th>\n",
       "      <th>Cleaned_Text</th>\n",
       "      <th>Tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0002bcb3da6cb337</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>0.58</td>\n",
       "      <td>cocksucker before you piss around on my work</td>\n",
       "      <td>[cocksucker, piss, around, work]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0005c987bdfc9d4b</td>\n",
       "      <td>Hey... what is it..\\n@ | talk .\\nWhat is it......</td>\n",
       "      <td>0.05</td>\n",
       "      <td>hey what is it   talk  what is it an exclusive...</td>\n",
       "      <td>[hey, talk, exclusive, group, wp, talibanswho,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0007e25b2121310b</td>\n",
       "      <td>Bye! \\n\\nDon't look, come or think of comming ...</td>\n",
       "      <td>0.05</td>\n",
       "      <td>bye dont look come or think of comming back to...</td>\n",
       "      <td>[bye, dont, look, come, think, comming, back, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001810bf8c45bf5f</td>\n",
       "      <td>You are gay or antisemmitian? \\n\\nArchangel WH...</td>\n",
       "      <td>0.67</td>\n",
       "      <td>you are gay or antisemmitian archangel white t...</td>\n",
       "      <td>[gay, antisemmitian, archangel, white, tiger, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00190820581d90ce</td>\n",
       "      <td>FUCK YOUR FILTHY MOTHER IN THE ASS, DRY!</td>\n",
       "      <td>0.39</td>\n",
       "      <td>fuck your filthy mother in the ass dry</td>\n",
       "      <td>[fuck, filthy, mother, ass, dry]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>001956c382006abd</td>\n",
       "      <td>I'm Sorry \\n\\nI'm sorry I screwed around with ...</td>\n",
       "      <td>0.05</td>\n",
       "      <td>im sorry im sorry i screwed around with someon...</td>\n",
       "      <td>[im, sorry, im, sorry, screwed, around, someon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>001dc38a83d420cf</td>\n",
       "      <td>GET FUCKED UP. GET FUCKEEED UP.  GOT A DRINK T...</td>\n",
       "      <td>0.29</td>\n",
       "      <td>get fucked up get fuckeeed up got a drink that...</td>\n",
       "      <td>[get, fucked, get, fuckeeed, got, drink, cant,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0020e7119b96eeeb</td>\n",
       "      <td>Stupid peace of shit stop deleting my stuff as...</td>\n",
       "      <td>0.58</td>\n",
       "      <td>stupid peace of shit stop deleting my stuff as...</td>\n",
       "      <td>[stupid, peace, shit, stop, deleting, stuff, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0020fd96ed3b8c8b</td>\n",
       "      <td>#ERROR!</td>\n",
       "      <td>0.39</td>\n",
       "      <td>tony sidaway is obviously a fistfuckee he love...</td>\n",
       "      <td>[tony, sidaway, obviously, fistfuckee, loves, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0021fe88bc4da3e6</td>\n",
       "      <td>My Band Page's deletion. You thought I was gon...</td>\n",
       "      <td>0.29</td>\n",
       "      <td>my band pages deletion you thought i was gone ...</td>\n",
       "      <td>[band, pages, deletion, thought, gone, deletin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  \\\n",
       "0  0002bcb3da6cb337       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK   \n",
       "1  0005c987bdfc9d4b  Hey... what is it..\\n@ | talk .\\nWhat is it......   \n",
       "2  0007e25b2121310b  Bye! \\n\\nDon't look, come or think of comming ...   \n",
       "3  001810bf8c45bf5f  You are gay or antisemmitian? \\n\\nArchangel WH...   \n",
       "4  00190820581d90ce           FUCK YOUR FILTHY MOTHER IN THE ASS, DRY!   \n",
       "5  001956c382006abd  I'm Sorry \\n\\nI'm sorry I screwed around with ...   \n",
       "6  001dc38a83d420cf  GET FUCKED UP. GET FUCKEEED UP.  GOT A DRINK T...   \n",
       "7  0020e7119b96eeeb  Stupid peace of shit stop deleting my stuff as...   \n",
       "8  0020fd96ed3b8c8b                                            #ERROR!   \n",
       "9  0021fe88bc4da3e6  My Band Page's deletion. You thought I was gon...   \n",
       "\n",
       "   target                                       Cleaned_Text  \\\n",
       "0    0.58       cocksucker before you piss around on my work   \n",
       "1    0.05  hey what is it   talk  what is it an exclusive...   \n",
       "2    0.05  bye dont look come or think of comming back to...   \n",
       "3    0.67  you are gay or antisemmitian archangel white t...   \n",
       "4    0.39             fuck your filthy mother in the ass dry   \n",
       "5    0.05  im sorry im sorry i screwed around with someon...   \n",
       "6    0.29  get fucked up get fuckeeed up got a drink that...   \n",
       "7    0.58  stupid peace of shit stop deleting my stuff as...   \n",
       "8    0.39  tony sidaway is obviously a fistfuckee he love...   \n",
       "9    0.29  my band pages deletion you thought i was gone ...   \n",
       "\n",
       "                                      Tokenized_text  \n",
       "0                   [cocksucker, piss, around, work]  \n",
       "1  [hey, talk, exclusive, group, wp, talibanswho,...  \n",
       "2  [bye, dont, look, come, think, comming, back, ...  \n",
       "3  [gay, antisemmitian, archangel, white, tiger, ...  \n",
       "4                   [fuck, filthy, mother, ass, dry]  \n",
       "5  [im, sorry, im, sorry, screwed, around, someon...  \n",
       "6  [get, fucked, get, fuckeeed, got, drink, cant,...  \n",
       "7  [stupid, peace, shit, stop, deleting, stuff, a...  \n",
       "8  [tony, sidaway, obviously, fistfuckee, loves, ...  \n",
       "9  [band, pages, deletion, thought, gone, deletin...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../input/jigsaw-train-csv/Jigsaw_Competition_trai.csv')\n",
    "df.drop(['toxic','severe_toxic','obscene','threat','insult','identity_hate','Unnamed: 0','text_cleaning'],inplace=True,axis=1)\n",
    "def tokenize(text):\n",
    "        token = nltk.word_tokenize(text)\n",
    "        return token\n",
    "\n",
    "df['Tokenized_text'] = df['Cleaned_Text'].apply(lambda x : tokenize(x))\n",
    "\n",
    "stpw = nltk.corpus.stopwords.words('english')\n",
    "def rmstp(text):\n",
    "    tt = [char.lower() for char in text if char not in stpw ]\n",
    "    return tt\n",
    "df['Tokenized_text'] = df['Tokenized_text'].apply(lambda x:rmstp(x))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bdc1615",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-09T18:41:23.719571Z",
     "iopub.status.busy": "2022-01-09T18:41:23.718664Z",
     "iopub.status.idle": "2022-01-09T18:42:09.022561Z",
     "shell.execute_reply": "2022-01-09T18:42:09.023153Z",
     "shell.execute_reply.started": "2022-01-09T17:58:59.745444Z"
    },
    "papermill": {
     "duration": 45.325387,
     "end_time": "2022-01-09T18:42:09.023380",
     "exception": false,
     "start_time": "2022-01-09T18:41:23.697993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 id                                       comment_text  \\\n",
      "0  0002bcb3da6cb337       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK   \n",
      "1  0005c987bdfc9d4b  Hey... what is it..\\n@ | talk .\\nWhat is it......   \n",
      "2  0007e25b2121310b  Bye! \\n\\nDon't look, come or think of comming ...   \n",
      "3  001810bf8c45bf5f  You are gay or antisemmitian? \\n\\nArchangel WH...   \n",
      "4  00190820581d90ce           FUCK YOUR FILTHY MOTHER IN THE ASS, DRY!   \n",
      "\n",
      "   target                                       Cleaned_Text  \\\n",
      "0    0.58       cocksucker before you piss around on my work   \n",
      "1    0.05  hey what is it   talk  what is it an exclusive...   \n",
      "2    0.05  bye dont look come or think of comming back to...   \n",
      "3    0.67  you are gay or antisemmitian archangel white t...   \n",
      "4    0.39             fuck your filthy mother in the ass dry   \n",
      "\n",
      "                                      Tokenized_text  \\\n",
      "0                   [cocksucker, piss, around, work]   \n",
      "1  [hey, talk, exclusive, group, wp, talibanswho,...   \n",
      "2  [bye, dont, look, come, think, comming, back, ...   \n",
      "3  [gay, antisemmitian, archangel, white, tiger, ...   \n",
      "4                   [fuck, filthy, mother, ass, dry]   \n",
      "\n",
      "                                          lemmatized  \n",
      "0                   [cocksucker, piss, around, work]  \n",
      "1  [hey, talk, exclusive, group, wp, talibanswho,...  \n",
      "2  [bye, dont, look, come, think, comming, back, ...  \n",
      "3  [gay, antisemmitian, archangel, white, tiger, ...  \n",
      "4                    [fuck, filthy, mother, as, dry]  \n"
     ]
    }
   ],
   "source": [
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "def rmstmer(text):\n",
    "    tt = [lemmatizer.lemmatize(char,tag_map[j[0]]) for char,j in pos_tag(text)]\n",
    "    return tt\n",
    "df['lemmatized'] = df['Tokenized_text'].apply(lambda x:rmstmer(x))\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ef72b1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-09T18:42:09.057968Z",
     "iopub.status.busy": "2022-01-09T18:42:09.056854Z",
     "iopub.status.idle": "2022-01-09T18:42:52.837423Z",
     "shell.execute_reply": "2022-01-09T18:42:52.836749Z",
     "shell.execute_reply.started": "2022-01-09T17:59:44.045670Z"
    },
    "papermill": {
     "duration": 43.801453,
     "end_time": "2022-01-09T18:42:52.837613",
     "exception": false,
     "start_time": "2022-01-09T18:42:09.036160",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(analyzer=rmstmer,ngram_range = (1,1),max_df=0.8,min_df=3)\n",
    "xvect = vect.fit_transform(df['lemmatized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f197bc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-09T18:42:52.867596Z",
     "iopub.status.busy": "2022-01-09T18:42:52.866854Z",
     "iopub.status.idle": "2022-01-09T18:47:46.671747Z",
     "shell.execute_reply": "2022-01-09T18:47:46.672711Z",
     "shell.execute_reply.started": "2022-01-09T18:00:26.891316Z"
    },
    "papermill": {
     "duration": 293.822188,
     "end_time": "2022-01-09T18:47:46.673076",
     "exception": false,
     "start_time": "2022-01-09T18:42:52.850888",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7352194584879352"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df['target']\n",
    "Xvect = pd.DataFrame(xvect.toarray())\n",
    "Xvect.shape\n",
    "model_lr = LinearRegression()\n",
    "model_lr.fit(Xvect,y)\n",
    "model_lr.score(Xvect,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f803e15a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-09T18:47:46.735463Z",
     "iopub.status.busy": "2022-01-09T18:47:46.732338Z",
     "iopub.status.idle": "2022-01-09T18:50:43.931798Z",
     "shell.execute_reply": "2022-01-09T18:50:43.932314Z",
     "shell.execute_reply.started": "2022-01-09T18:11:53.915642Z"
    },
    "papermill": {
     "duration": 177.235362,
     "end_time": "2022-01-09T18:50:43.932616",
     "exception": false,
     "start_time": "2022-01-09T18:47:46.697254",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7537/7537 [02:57<00:00, 42.56it/s]\n"
     ]
    }
   ],
   "source": [
    "pred_df = pd.read_csv('../input/jigsaw-toxic-severity-rating/comments_to_score.csv')\n",
    "pred_df.head()\n",
    "sub_df =  pd.read_csv('../input/jigsaw-toxic-severity-rating/sample_submission.csv')\n",
    "sub_df.shape\n",
    "res = []\n",
    "for i in tqdm(pred_df['text']):\n",
    "    sample_1 = vect.transform([' '.join(rmstp(tokenize(text_cleaning(i))))])\n",
    "    res.append(model_lr.predict(sample_1.toarray()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3b6e6e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-09T18:50:44.639028Z",
     "iopub.status.busy": "2022-01-09T18:50:44.637897Z",
     "iopub.status.idle": "2022-01-09T18:50:44.642757Z",
     "shell.execute_reply": "2022-01-09T18:50:44.642176Z",
     "shell.execute_reply.started": "2022-01-09T18:14:51.875367Z"
    },
    "papermill": {
     "duration": 0.368475,
     "end_time": "2022-01-09T18:50:44.642918",
     "exception": false,
     "start_time": "2022-01-09T18:50:44.274443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>114890</td>\n",
       "      <td>0.320087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>732895</td>\n",
       "      <td>0.189670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1139051</td>\n",
       "      <td>0.360781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1434512</td>\n",
       "      <td>0.329932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2084821</td>\n",
       "      <td>0.180743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7532</th>\n",
       "      <td>504235362</td>\n",
       "      <td>0.401490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7533</th>\n",
       "      <td>504235566</td>\n",
       "      <td>0.209337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7534</th>\n",
       "      <td>504308177</td>\n",
       "      <td>0.331065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7535</th>\n",
       "      <td>504570375</td>\n",
       "      <td>0.431260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7536</th>\n",
       "      <td>504598250</td>\n",
       "      <td>0.390490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7537 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      comment_id     score\n",
       "0         114890  0.320087\n",
       "1         732895  0.189670\n",
       "2        1139051  0.360781\n",
       "3        1434512  0.329932\n",
       "4        2084821  0.180743\n",
       "...          ...       ...\n",
       "7532   504235362  0.401490\n",
       "7533   504235566  0.209337\n",
       "7534   504308177  0.331065\n",
       "7535   504570375  0.431260\n",
       "7536   504598250  0.390490\n",
       "\n",
       "[7537 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df['score'] = [i[0] for i in res]\n",
    "sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cd0aae9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-09T18:50:45.333237Z",
     "iopub.status.busy": "2022-01-09T18:50:45.332178Z",
     "iopub.status.idle": "2022-01-09T18:50:45.370007Z",
     "shell.execute_reply": "2022-01-09T18:50:45.369288Z",
     "shell.execute_reply.started": "2022-01-09T18:14:51.899145Z"
    },
    "papermill": {
     "duration": 0.383517,
     "end_time": "2022-01-09T18:50:45.370181",
     "exception": false,
     "start_time": "2022-01-09T18:50:44.986664",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub_df.to_csv('./submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e75f08d",
   "metadata": {
    "papermill": {
     "duration": 0.340443,
     "end_time": "2022-01-09T18:50:46.055612",
     "exception": false,
     "start_time": "2022-01-09T18:50:45.715169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 589.822855,
   "end_time": "2022-01-09T18:50:48.019012",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-01-09T18:40:58.196157",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
